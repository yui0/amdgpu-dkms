diff -Nuarp amdgpu-17.50-511655.el7.orig/Makefile amdgpu-17.50-511655.el7/Makefile
--- amdgpu-17.50-511655.el7.orig/Makefile	2017-12-02 06:30:56.000000000 +0900
+++ amdgpu-17.50-511655.el7/Makefile	2018-03-01 17:11:14.462119000 +0900
@@ -14,6 +14,8 @@ OS_NAME = "rhel"
 OS_VERSION = "$(shell sed -n -e 's/[^0-9]*\([0-9]*.[0-9]*\).*/\1/p' /etc/centos-release-upstream)"
 endif
 
+OS_VERSION = "7,3"
+
 ifeq ("ubuntu",$(OS_NAME))
 subdir-ccflags-y += -DOS_NAME_UBUNTU
 else ifeq ("rhel",$(OS_NAME))
diff -Nuarp amdgpu-17.50-511655.el7.orig/amd/amdgpu/amdgpu_amdkfd_gpuvm.c amdgpu-17.50-511655.el7/amd/amdgpu/amdgpu_amdkfd_gpuvm.c
--- amdgpu-17.50-511655.el7.orig/amd/amdgpu/amdgpu_amdkfd_gpuvm.c	2017-12-02 06:30:56.000000000 +0900
+++ amdgpu-17.50-511655.el7/amd/amdgpu/amdgpu_amdkfd_gpuvm.c	2018-03-02 15:51:55.295680000 +0900
@@ -607,7 +607,11 @@ static int init_user_pages(struct kgd_me
 
 release_out:
 	if (ret)
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+		release_pages(mem->user_pages, bo->tbo.ttm->num_pages);
+#else
 		release_pages(mem->user_pages, bo->tbo.ttm->num_pages, 0);
+#endif
 free_out:
 #if LINUX_VERSION_CODE < KERNEL_VERSION(4, 12, 0)
 	drm_free_large(mem->user_pages);
@@ -1198,8 +1202,11 @@ int amdgpu_amdkfd_gpuvm_free_memory_of_g
 	if (mem->user_pages) {
 		pr_debug("%s: Freeing user_pages array\n", __func__);
 		if (mem->user_pages[0])
-			release_pages(mem->user_pages,
-				      mem->bo->tbo.ttm->num_pages, 0);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+			release_pages(mem->user_pages, mem->bo->tbo.ttm->num_pages);
+#else
+			release_pages(mem->user_pages, mem->bo->tbo.ttm->num_pages, 0);
+#endif
 #if LINUX_VERSION_CODE < KERNEL_VERSION(4, 12, 0)
 		drm_free_large(mem->user_pages);
 #else
@@ -2019,8 +2026,11 @@ static int update_invalid_user_pages(str
 				return -ENOMEM;
 			}
 		} else if (mem->user_pages[0]) {
-			release_pages(mem->user_pages,
-				      bo->tbo.ttm->num_pages, 0);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+			release_pages(mem->user_pages, bo->tbo.ttm->num_pages);
+#else
+			release_pages(mem->user_pages, bo->tbo.ttm->num_pages, 0);
+#endif
 		}
 
 		/* Get updated user pages */
diff -Nuarp amdgpu-17.50-511655.el7.orig/amd/amdgpu/amdgpu_connectors.c amdgpu-17.50-511655.el7/amd/amdgpu/amdgpu_connectors.c
--- amdgpu-17.50-511655.el7.orig/amd/amdgpu/amdgpu_connectors.c	2017-12-02 06:30:56.000000000 +0900
+++ amdgpu-17.50-511655.el7/amd/amdgpu/amdgpu_connectors.c	2018-03-01 17:20:43.540734000 +0900
@@ -240,6 +240,9 @@ amdgpu_connector_update_scratch_regs(str
 			break;
 
 		encoder = drm_encoder_find(connector->dev,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+					NULL,
+#endif
 					connector->encoder_ids[i]);
 		if (!encoder)
 			continue;
@@ -265,6 +268,9 @@ amdgpu_connector_find_encoder(struct drm
 		if (connector->encoder_ids[i] == 0)
 			break;
 		encoder = drm_encoder_find(connector->dev,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+					NULL,
+#endif
 					connector->encoder_ids[i]);
 		if (!encoder)
 			continue;
@@ -380,7 +386,11 @@ amdgpu_connector_best_single_encoder(str
 
 	/* pick the encoder ids */
 	if (enc_id)
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+		return drm_encoder_find(connector->dev, NULL, enc_id);
+#else
 		return drm_encoder_find(connector->dev, enc_id);
+#endif
 	return NULL;
 }
 
@@ -1091,7 +1101,11 @@ amdgpu_connector_dvi_detect(struct drm_c
 			if (connector->encoder_ids[i] == 0)
 				break;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+			encoder = drm_encoder_find(connector->dev, NULL, connector->encoder_ids[i]);
+#else
 			encoder = drm_encoder_find(connector->dev, connector->encoder_ids[i]);
+#endif
 			if (!encoder)
 				continue;
 
@@ -1148,7 +1162,11 @@ amdgpu_connector_dvi_encoder(struct drm_
 		if (connector->encoder_ids[i] == 0)
 			break;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+		encoder = drm_encoder_find(connector->dev, NULL, connector->encoder_ids[i]);
+#else
 		encoder = drm_encoder_find(connector->dev, connector->encoder_ids[i]);
+#endif
 		if (!encoder)
 			continue;
 
@@ -1167,7 +1185,11 @@ amdgpu_connector_dvi_encoder(struct drm_
 	/* then check use digitial */
 	/* pick the first one */
 	if (enc_id)
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+		return drm_encoder_find(connector->dev, NULL, enc_id);
+#else
 		return drm_encoder_find(connector->dev, enc_id);
+#endif
 	return NULL;
 }
 
@@ -1310,8 +1332,11 @@ u16 amdgpu_connector_encoder_get_dp_brid
 		if (connector->encoder_ids[i] == 0)
 			break;
 
-		encoder = drm_encoder_find(connector->dev,
-					connector->encoder_ids[i]);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+		encoder = drm_encoder_find(connector->dev, NULL, connector->encoder_ids[i]);
+#else
+		encoder = drm_encoder_find(connector->dev, connector->encoder_ids[i]);
+#endif
 		if (!encoder)
 			continue;
 
@@ -1339,8 +1364,11 @@ static bool amdgpu_connector_encoder_is_
 	for (i = 0; i < DRM_CONNECTOR_MAX_ENCODER; i++) {
 		if (connector->encoder_ids[i] == 0)
 			break;
-		encoder = drm_encoder_find(connector->dev,
-					connector->encoder_ids[i]);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+		encoder = drm_encoder_find(connector->dev, NULL, connector->encoder_ids[i]);
+#else
+		encoder = drm_encoder_find(connector->dev, connector->encoder_ids[i]);
+#endif
 		if (!encoder)
 			continue;
 
diff -Nuarp amdgpu-17.50-511655.el7.orig/amd/amdgpu/amdgpu_cs.c amdgpu-17.50-511655.el7/amd/amdgpu/amdgpu_cs.c
--- amdgpu-17.50-511655.el7.orig/amd/amdgpu/amdgpu_cs.c	2017-12-02 06:30:56.000000000 +0900
+++ amdgpu-17.50-511655.el7/amd/amdgpu/amdgpu_cs.c	2018-03-01 18:37:10.623347000 +0900
@@ -572,8 +572,11 @@ static int amdgpu_cs_parser_bos(struct a
 				 * invalidated it. Free it and try again
 				 */
 				release_pages(e->user_pages,
-					      bo->tbo.ttm->num_pages,
-					      false);
+					      bo->tbo.ttm->num_pages
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 14, 0)
+					      ,false
+#endif
+				);
 #if LINUX_VERSION_CODE < KERNEL_VERSION(4, 12, 0)
 				drm_free_large(e->user_pages);
 #else
@@ -717,8 +720,11 @@ error_free_pages:
 				continue;
 
 			release_pages(e->user_pages,
-				      e->robj->tbo.ttm->num_pages,
-				      false);
+				      e->robj->tbo.ttm->num_pages
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 14, 0)
+				      ,false
+#endif
+			);
 #if LINUX_VERSION_CODE < KERNEL_VERSION(4, 12, 0)
 			drm_free_large(e->user_pages);
 #else
diff -Nuarp amdgpu-17.50-511655.el7.orig/amd/amdgpu/amdgpu_drv.c amdgpu-17.50-511655.el7/amd/amdgpu/amdgpu_drv.c
--- amdgpu-17.50-511655.el7.orig/amd/amdgpu/amdgpu_drv.c	2017-12-02 06:30:56.000000000 +0900
+++ amdgpu-17.50-511655.el7/amd/amdgpu/amdgpu_drv.c	2018-03-01 17:13:18.494981000 +0900
@@ -826,7 +826,9 @@ static struct drm_driver kms_driver = {
 	.open = amdgpu_driver_open_kms,
 	.postclose = amdgpu_driver_postclose_kms,
 	.lastclose = amdgpu_driver_lastclose_kms,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 14, 0)
 	.set_busid = drm_pci_set_busid,
+#endif
 	.unload = amdgpu_driver_unload_kms,
 	.get_vblank_counter = kcl_amdgpu_get_vblank_counter_kms,
 	.enable_vblank = kcl_amdgpu_enable_vblank_kms,
diff -Nuarp amdgpu-17.50-511655.el7.orig/amd/amdgpu/amdgpu_fb.c amdgpu-17.50-511655.el7/amd/amdgpu/amdgpu_fb.c
--- amdgpu-17.50-511655.el7.orig/amd/amdgpu/amdgpu_fb.c	2017-12-02 06:30:56.000000000 +0900
+++ amdgpu-17.50-511655.el7/amd/amdgpu/amdgpu_fb.c	2018-03-01 18:12:46.611312000 +0900
@@ -357,8 +357,10 @@ static void amdgpu_crtc_fb_gamma_get(str
 }
 
 static const struct drm_fb_helper_funcs amdgpu_fb_helper_funcs = {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 14, 0)
 	.gamma_set = amdgpu_crtc_fb_gamma_set,
 	.gamma_get = amdgpu_crtc_fb_gamma_get,
+#endif
 	.fb_probe = amdgpufb_create,
 };
 
diff -Nuarp amdgpu-17.50-511655.el7.orig/amd/amdgpu/amdgpu_fence.c amdgpu-17.50-511655.el7/amd/amdgpu/amdgpu_fence.c
--- amdgpu-17.50-511655.el7.orig/amd/amdgpu/amdgpu_fence.c	2017-12-02 06:30:56.000000000 +0900
+++ amdgpu-17.50-511655.el7/amd/amdgpu/amdgpu_fence.c	2018-03-01 18:09:49.472765000 +0900
@@ -242,12 +242,19 @@ void amdgpu_fence_process(struct amdgpu_
  *
  * Checks for fence activity.
  */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+static void amdgpu_fence_fallback(struct timer_list *t)
+{
+	struct amdgpu_ring *ring = from_timer(ring, t, fence_drv.fallback_timer);
+	amdgpu_fence_process(ring);
+}
+#else
 static void amdgpu_fence_fallback(unsigned long arg)
 {
 	struct amdgpu_ring *ring = (void *)arg;
-
 	amdgpu_fence_process(ring);
 }
+#endif
 
 /**
  * amdgpu_fence_wait_empty - wait for all fences to signal
@@ -260,7 +267,11 @@ static void amdgpu_fence_fallback(unsign
  */
 int amdgpu_fence_wait_empty(struct amdgpu_ring *ring)
 {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	uint64_t seq = READ_ONCE(ring->fence_drv.sync_seq);
+#else
 	uint64_t seq = ACCESS_ONCE(ring->fence_drv.sync_seq);
+#endif
 	struct dma_fence *fence, **ptr;
 	int r;
 
@@ -300,7 +311,11 @@ unsigned amdgpu_fence_count_emitted(stru
 	amdgpu_fence_process(ring);
 	emitted = 0x100000000ull;
 	emitted -= atomic_read(&ring->fence_drv.last_seq);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	emitted += READ_ONCE(ring->fence_drv.sync_seq);
+#else
 	emitted += ACCESS_ONCE(ring->fence_drv.sync_seq);
+#endif
 	return lower_32_bits(emitted);
 }
 
@@ -372,8 +387,12 @@ int amdgpu_fence_driver_init_ring(struct
 	atomic_set(&ring->fence_drv.last_seq, 0);
 	ring->fence_drv.initialized = false;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	timer_setup(&ring->fence_drv.fallback_timer, amdgpu_fence_fallback, 0);
+#else
 	setup_timer(&ring->fence_drv.fallback_timer, amdgpu_fence_fallback,
 		    (unsigned long)ring);
+#endif
 
 	ring->fence_drv.num_fences_mask = num_hw_submission * 2 - 1;
 	spin_lock_init(&ring->fence_drv.lock);
diff -Nuarp amdgpu-17.50-511655.el7.orig/amd/amdgpu/amdgpu_gem.c amdgpu-17.50-511655.el7/amd/amdgpu/amdgpu_gem.c
--- amdgpu-17.50-511655.el7.orig/amd/amdgpu/amdgpu_gem.c	2017-12-02 06:30:56.000000000 +0900
+++ amdgpu-17.50-511655.el7/amd/amdgpu/amdgpu_gem.c	2018-03-01 18:35:10.484731000 +0900
@@ -470,7 +470,11 @@ int amdgpu_gem_userptr_ioctl(struct drm_
 	return 0;
 
 free_pages:
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	release_pages(bo->tbo.ttm->pages, bo->tbo.ttm->num_pages);
+#else
 	release_pages(bo->tbo.ttm->pages, bo->tbo.ttm->num_pages, false);
+#endif
 
 release_object:
 	kcl_drm_gem_object_put_unlocked(gobj);
diff -Nuarp amdgpu-17.50-511655.el7.orig/amd/amdgpu/amdgpu_irq.c amdgpu-17.50-511655.el7/amd/amdgpu/amdgpu_irq.c
--- amdgpu-17.50-511655.el7.orig/amd/amdgpu/amdgpu_irq.c	2017-12-02 06:30:56.000000000 +0900
+++ amdgpu-17.50-511655.el7/amd/amdgpu/amdgpu_irq.c	2018-03-02 15:38:34.792215000 +0900
@@ -277,7 +277,9 @@ void amdgpu_irq_fini(struct amdgpu_devic
 {
 	unsigned i, j;
 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 14, 0)
 	drm_vblank_cleanup(adev->ddev);
+#endif
 	if (adev->irq.installed) {
 		drm_irq_uninstall(adev->ddev);
 		adev->irq.installed = false;
diff -Nuarp amdgpu-17.50-511655.el7.orig/amd/amdgpu/amdgpu_mn.c amdgpu-17.50-511655.el7/amd/amdgpu/amdgpu_mn.c
--- amdgpu-17.50-511655.el7.orig/amd/amdgpu/amdgpu_mn.c	2017-12-02 06:30:56.000000000 +0900
+++ amdgpu-17.50-511655.el7/amd/amdgpu/amdgpu_mn.c	2018-03-02 16:00:06.032096000 +0900
@@ -53,7 +53,11 @@ struct amdgpu_mn {
 
 	/* objects protected by lock */
 	struct rw_semaphore	lock;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	struct rb_root_cached	objects;
+#else
 	struct rb_root		objects;
+#endif
 	struct mutex		read_lock;
 	atomic_t		recursion;
 };
@@ -80,8 +84,11 @@ static void amdgpu_mn_destroy(struct wor
 	mutex_lock(&adev->mn_lock);
 	down_write(&rmn->lock);
 	hash_del(&rmn->node);
-	rbtree_postorder_for_each_entry_safe(node, next_node, &rmn->objects,
-					     it.rb) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	rbtree_postorder_for_each_entry_safe(node, next_node, &rmn->objects.rb_root, it.rb) {
+#else
+	rbtree_postorder_for_each_entry_safe(node, next_node, &rmn->objects, it.rb) {
+#endif
 		list_for_each_entry_safe(bo, next_bo, &node->bos, mn_list) {
 			bo->mn = NULL;
 			list_del_init(&bo->mn_list);
@@ -429,7 +436,11 @@ struct amdgpu_mn *amdgpu_mn_get(struct a
 	rmn->type = type;
 	rmn->mn.ops = &amdgpu_mn_ops[type];
 	init_rwsem(&rmn->lock);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	rmn->objects = RB_ROOT_CACHED;
+#else
 	rmn->objects = RB_ROOT;
+#endif
 	mutex_init(&rmn->read_lock);
 	atomic_set(&rmn->recursion, 0);
 
diff -Nuarp amdgpu-17.50-511655.el7.orig/amd/amdgpu/amdgpu_ttm.c amdgpu-17.50-511655.el7/amd/amdgpu/amdgpu_ttm.c
--- amdgpu-17.50-511655.el7.orig/amd/amdgpu/amdgpu_ttm.c	2017-12-02 06:30:56.000000000 +0900
+++ amdgpu-17.50-511655.el7/amd/amdgpu/amdgpu_ttm.c	2018-03-01 18:11:29.562649000 +0900
@@ -738,7 +738,11 @@ int amdgpu_ttm_tt_get_user_pages(struct
 	return 0;
 
 release_pages:
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	release_pages(pages, pinned);
+#else
 	release_pages(pages, pinned, 0);
+#endif
 	up_read(&mm->mmap_sem);
 	return r;
 }
diff -Nuarp amdgpu-17.50-511655.el7.orig/amd/amdgpu/amdgpu_ttm.h amdgpu-17.50-511655.el7/amd/amdgpu/amdgpu_ttm.h
--- amdgpu-17.50-511655.el7.orig/amd/amdgpu/amdgpu_ttm.h	2017-12-02 06:30:56.000000000 +0900
+++ amdgpu-17.50-511655.el7/amd/amdgpu/amdgpu_ttm.h	2018-03-01 17:09:27.126800000 +0900
@@ -24,7 +24,7 @@
 #ifndef __AMDGPU_TTM_H__
 #define __AMDGPU_TTM_H__
 
-#include "gpu_scheduler.h"
+#include "../scheduler/gpu_scheduler.h"
 
 #define AMDGPU_PL_GDS		(TTM_PL_PRIV + 0)
 #define AMDGPU_PL_GWS		(TTM_PL_PRIV + 1)
diff -Nuarp amdgpu-17.50-511655.el7.orig/amd/amdgpu/amdgpu_vm.c amdgpu-17.50-511655.el7/amd/amdgpu/amdgpu_vm.c
--- amdgpu-17.50-511655.el7.orig/amd/amdgpu/amdgpu_vm.c	2017-12-02 06:30:56.000000000 +0900
+++ amdgpu-17.50-511655.el7/amd/amdgpu/amdgpu_vm.c	2018-03-02 15:33:12.687596000 +0900
@@ -32,6 +32,7 @@
 #endif
 #if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 7, 0)
 #include <linux/interval_tree_generic.h>
+//#include "kcl/kcl_interval_tree_generic.h"
 #endif
 #include <drm/drmP.h>
 #include <drm/amdgpu_drm.h>
@@ -2588,7 +2589,11 @@ int amdgpu_vm_init(struct amdgpu_device
 	u64 flags;
 	uint64_t init_pde_value = 0;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	vm->va = RB_ROOT_CACHED;
+#else
 	vm->va = RB_ROOT;
+#endif
 	vm->client_id = atomic64_inc_return(&adev->vm_manager.client_counter);
 	for (i = 0; i < AMDGPU_MAX_VMHUBS; i++)
 		vm->reserved_vmid[i] = NULL;
@@ -2752,10 +2757,20 @@ void amdgpu_vm_fini(struct amdgpu_device
 
 	amd_sched_entity_fini(vm->entity.sched, &vm->entity);
 
-	if (!RB_EMPTY_ROOT(&vm->va)) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	if (!RB_EMPTY_ROOT(&vm->va.rb_root))
+#else
+	if (!RB_EMPTY_ROOT(&vm->va))
+#endif
+	{
 		dev_err(adev->dev, "still active bo inside vm\n");
 	}
-	rbtree_postorder_for_each_entry_safe(mapping, tmp, &vm->va, rb) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	rbtree_postorder_for_each_entry_safe(mapping, tmp, &vm->va.rb_root, rb)
+#else
+	rbtree_postorder_for_each_entry_safe(mapping, tmp, &vm->va, rb)
+#endif
+	{
 		list_del(&mapping->list);
 		amdgpu_vm_it_remove(mapping, &vm->va);
 		kfree(mapping);
diff -Nuarp amdgpu-17.50-511655.el7.orig/amd/amdgpu/amdgpu_vm.h amdgpu-17.50-511655.el7/amd/amdgpu/amdgpu_vm.h
--- amdgpu-17.50-511655.el7.orig/amd/amdgpu/amdgpu_vm.h	2017-12-02 06:30:56.000000000 +0900
+++ amdgpu-17.50-511655.el7/amd/amdgpu/amdgpu_vm.h	2018-03-02 15:26:19.689861000 +0900
@@ -121,7 +121,11 @@ struct amdgpu_vm_pt {
 
 struct amdgpu_vm {
 	/* tree of virtual addresses mapped */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	struct rb_root_cached	va;
+#else
 	struct rb_root		va;
+#endif
 
 	/* protecting invalidated */
 	spinlock_t		status_lock;
diff -Nuarp amdgpu-17.50-511655.el7.orig/amd/amdgpu/dce_v10_0.c amdgpu-17.50-511655.el7/amd/amdgpu/dce_v10_0.c
--- amdgpu-17.50-511655.el7.orig/amd/amdgpu/dce_v10_0.c	2017-12-02 06:30:56.000000000 +0900
+++ amdgpu-17.50-511655.el7/amd/amdgpu/dce_v10_0.c	2018-03-02 15:45:46.668225000 +0900
@@ -1739,7 +1739,11 @@ static void dce_v10_0_afmt_setmode(struc
 	dce_v10_0_audio_write_sad_regs(encoder);
 	dce_v10_0_audio_write_latency_fields(encoder, mode);
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	err = drm_hdmi_avi_infoframe_from_display_mode(&frame, mode, false);
+#else
 	err = drm_hdmi_avi_infoframe_from_display_mode(&frame, mode);
+#endif
 	if (err < 0) {
 		DRM_ERROR("failed to setup AVI infoframe: %zd\n", err);
 		return;
@@ -2754,7 +2758,9 @@ static const struct drm_crtc_helper_func
 	.mode_set_base_atomic = dce_v10_0_crtc_set_base_atomic,
 	.prepare = dce_v10_0_crtc_prepare,
 	.commit = dce_v10_0_crtc_commit,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 14, 0)
 	.load_lut = dce_v10_0_crtc_load_lut,
+#endif
 	.disable = dce_v10_0_crtc_disable,
 };
 
diff -Nuarp amdgpu-17.50-511655.el7.orig/amd/amdgpu/dce_v11_0.c amdgpu-17.50-511655.el7/amd/amdgpu/dce_v11_0.c
--- amdgpu-17.50-511655.el7.orig/amd/amdgpu/dce_v11_0.c	2017-12-02 06:30:56.000000000 +0900
+++ amdgpu-17.50-511655.el7/amd/amdgpu/dce_v11_0.c	2018-03-02 15:45:50.571403000 +0900
@@ -1778,7 +1778,11 @@ static void dce_v11_0_afmt_setmode(struc
 	dce_v11_0_audio_write_sad_regs(encoder);
 	dce_v11_0_audio_write_latency_fields(encoder, mode);
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	err = drm_hdmi_avi_infoframe_from_display_mode(&frame, mode, false);
+#else
 	err = drm_hdmi_avi_infoframe_from_display_mode(&frame, mode);
+#endif
 	if (err < 0) {
 		DRM_ERROR("failed to setup AVI infoframe: %zd\n", err);
 		return;
@@ -2857,7 +2861,9 @@ static const struct drm_crtc_helper_func
 	.mode_set_base_atomic = dce_v11_0_crtc_set_base_atomic,
 	.prepare = dce_v11_0_crtc_prepare,
 	.commit = dce_v11_0_crtc_commit,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 14, 0)
 	.load_lut = dce_v11_0_crtc_load_lut,
+#endif
 	.disable = dce_v11_0_crtc_disable,
 };
 
diff -Nuarp amdgpu-17.50-511655.el7.orig/amd/amdgpu/dce_v6_0.c amdgpu-17.50-511655.el7/amd/amdgpu/dce_v6_0.c
--- amdgpu-17.50-511655.el7.orig/amd/amdgpu/dce_v6_0.c	2017-12-02 06:30:56.000000000 +0900
+++ amdgpu-17.50-511655.el7/amd/amdgpu/dce_v6_0.c	2018-03-02 15:37:09.159741000 +0900
@@ -1487,7 +1487,11 @@ static void dce_v6_0_audio_set_avi_infof
 	ssize_t err;
 	u32 tmp;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	err = drm_hdmi_avi_infoframe_from_display_mode(&frame, mode, false);
+#else
 	err = drm_hdmi_avi_infoframe_from_display_mode(&frame, mode);
+#endif
 	if (err < 0) {
 		DRM_ERROR("failed to setup AVI infoframe: %zd\n", err);
 		return;
@@ -2640,7 +2644,9 @@ static const struct drm_crtc_helper_func
 	.mode_set_base_atomic = dce_v6_0_crtc_set_base_atomic,
 	.prepare = dce_v6_0_crtc_prepare,
 	.commit = dce_v6_0_crtc_commit,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 14, 0)
 	.load_lut = dce_v6_0_crtc_load_lut,
+#endif
 	.disable = dce_v6_0_crtc_disable,
 };
 
diff -Nuarp amdgpu-17.50-511655.el7.orig/amd/amdgpu/dce_v8_0.c amdgpu-17.50-511655.el7/amd/amdgpu/dce_v8_0.c
--- amdgpu-17.50-511655.el7.orig/amd/amdgpu/dce_v8_0.c	2017-12-02 06:30:56.000000000 +0900
+++ amdgpu-17.50-511655.el7/amd/amdgpu/dce_v8_0.c	2018-03-02 15:35:48.116987000 +0900
@@ -1675,7 +1675,11 @@ static void dce_v8_0_afmt_setmode(struct
 	dce_v8_0_audio_write_sad_regs(encoder);
 	dce_v8_0_audio_write_latency_fields(encoder, mode);
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	err = drm_hdmi_avi_infoframe_from_display_mode(&frame, mode, false);
+#else
 	err = drm_hdmi_avi_infoframe_from_display_mode(&frame, mode);
+#endif
 	if (err < 0) {
 		DRM_ERROR("failed to setup AVI infoframe: %zd\n", err);
 		return;
@@ -2665,7 +2669,9 @@ static const struct drm_crtc_helper_func
 	.mode_set_base_atomic = dce_v8_0_crtc_set_base_atomic,
 	.prepare = dce_v8_0_crtc_prepare,
 	.commit = dce_v8_0_crtc_commit,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 14, 0)
 	.load_lut = dce_v8_0_crtc_load_lut,
+#endif
 	.disable = dce_v8_0_crtc_disable,
 };
 
diff -Nuarp amdgpu-17.50-511655.el7.orig/amd/amdgpu/dce_virtual.c amdgpu-17.50-511655.el7/amd/amdgpu/dce_virtual.c
--- amdgpu-17.50-511655.el7.orig/amd/amdgpu/dce_virtual.c	2017-12-02 06:30:56.000000000 +0900
+++ amdgpu-17.50-511655.el7/amd/amdgpu/dce_virtual.c	2018-03-02 15:47:38.796946000 +0900
@@ -278,7 +278,9 @@ static const struct drm_crtc_helper_func
 	.mode_set_base_atomic = dce_virtual_crtc_set_base_atomic,
 	.prepare = dce_virtual_crtc_prepare,
 	.commit = dce_virtual_crtc_commit,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 14, 0)
 	.load_lut = dce_virtual_crtc_load_lut,
+#endif
 	.disable = dce_virtual_crtc_disable,
 };
 
@@ -336,7 +338,11 @@ dce_virtual_encoder(struct drm_connector
 		if (connector->encoder_ids[i] == 0)
 			break;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+		encoder = drm_encoder_find(connector->dev, NULL, connector->encoder_ids[i]);
+#else
 		encoder = drm_encoder_find(connector->dev, connector->encoder_ids[i]);
+#endif
 		if (!encoder)
 			continue;
 
@@ -346,7 +352,11 @@ dce_virtual_encoder(struct drm_connector
 
 	/* pick the first one */
 	if (enc_id)
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+		return drm_encoder_find(connector->dev, NULL, enc_id);
+#else
 		return drm_encoder_find(connector->dev, enc_id);
+#endif
 	return NULL;
 }
 
diff -Nuarp amdgpu-17.50-511655.el7.orig/amd/amdkcl/kcl_drm.c amdgpu-17.50-511655.el7/amd/amdkcl/kcl_drm.c
--- amdgpu-17.50-511655.el7.orig/amd/amdkcl/kcl_drm.c	2017-12-02 06:30:56.000000000 +0900
+++ amdgpu-17.50-511655.el7/amd/amdkcl/kcl_drm.c	2018-03-02 16:33:15.204973000 +0900
@@ -262,7 +262,11 @@ _kcl_drm_atomic_helper_update_legacy_mod
 	int i;
 
 	/* clear out existing links and update dpms */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	for_each_old_connector_in_state(old_state, connector, old_conn_state, i) {
+#else
 	for_each_connector_in_state(old_state, connector, old_conn_state, i) {
+#endif
 		if (connector->encoder) {
 			WARN_ON(!connector->encoder->crtc);
 
@@ -287,7 +291,11 @@ _kcl_drm_atomic_helper_update_legacy_mod
 	}
 
 	/* set new links */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	for_each_old_connector_in_state(old_state, connector, old_conn_state, i) {
+#else
 	for_each_connector_in_state(old_state, connector, old_conn_state, i) {
+#endif
 		if (!connector->state->crtc)
 			continue;
 
@@ -299,7 +307,11 @@ _kcl_drm_atomic_helper_update_legacy_mod
 	}
 
 	/* set legacy state in the crtc structure */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	for_each_old_crtc_in_state(old_state, crtc, old_crtc_state, i) {
+#else
 	for_each_crtc_in_state(old_state, crtc, old_crtc_state, i) {
+#endif
 		struct drm_plane *primary = crtc->primary;
 
 		crtc->mode = crtc->state->mode;
diff -Nuarp amdgpu-17.50-511655.el7.orig/amd/amdkcl/kcl_fence.c amdgpu-17.50-511655.el7/amd/amdkcl/kcl_fence.c
--- amdgpu-17.50-511655.el7.orig/amd/amdkcl/kcl_fence.c	2017-12-02 06:30:56.000000000 +0900
+++ amdgpu-17.50-511655.el7/amd/amdkcl/kcl_fence.c	2018-03-02 16:46:52.718343000 +0900
@@ -21,6 +21,9 @@
 #include <linux/slab.h>
 #include <kcl/kcl_fence.h>
 #include <kcl/kcl_rcupdate.h>
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+#include <linux/sched/signal.h>
+#endif
 #include "kcl_common.h"
 
 #define CREATE_TRACE_POINTS
diff -Nuarp amdgpu-17.50-511655.el7.orig/amd/amdkcl/symbols.c amdgpu-17.50-511655.el7/amd/amdkcl/symbols.c
--- amdgpu-17.50-511655.el7.orig/amd/amdkcl/symbols.c	1970-01-01 09:00:00.000000000 +0900
+++ amdgpu-17.50-511655.el7/amd/amdkcl/symbols.c	2018-03-02 21:23:28.689778000 +0900
@@ -0,0 +1 @@
+// auto generated by DKMS pre-build.sh
diff -Nuarp amdgpu-17.50-511655.el7.orig/amd/amdkfd/kfd_peerdirect.c amdgpu-17.50-511655.el7/amd/amdkfd/kfd_peerdirect.c
--- amdgpu-17.50-511655.el7.orig/amd/amdkfd/kfd_peerdirect.c	2017-12-02 06:30:56.000000000 +0900
+++ amdgpu-17.50-511655.el7/amd/amdkfd/kfd_peerdirect.c	2018-03-02 17:00:39.367205000 +0900
@@ -174,7 +174,11 @@ static void free_callback(void *client_p
 	/* amdkfd will free resources when we return from this callback.
 	 * Set flag to inform that there is nothing to do on "put_pages", etc.
 	 */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	WRITE_ONCE(mem_context->free_callback_called, 1);
+#else
 	ACCESS_ONCE(mem_context->free_callback_called) = 1;
+#endif
 }
 
 
@@ -359,7 +363,11 @@ static void amd_put_pages(struct sg_tabl
 	pr_debug("mem_context->p2p_info %p\n",
 				mem_context->p2p_info);
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	if (READ_ONCE(mem_context->free_callback_called)) {
+#else
 	if (ACCESS_ONCE(mem_context->free_callback_called)) {
+#endif
 		pr_debug("Free callback was called\n");
 		return;
 	}
diff -Nuarp amdgpu-17.50-511655.el7.orig/amd/amdkfd/kfd_priv.h amdgpu-17.50-511655.el7/amd/amdkfd/kfd_priv.h
--- amdgpu-17.50-511655.el7.orig/amd/amdkfd/kfd_priv.h	2017-12-02 06:30:56.000000000 +0900
+++ amdgpu-17.50-511655.el7/amd/amdkfd/kfd_priv.h	2018-03-02 16:50:21.839040000 +0900
@@ -726,7 +726,11 @@ struct kfd_process {
 	size_t signal_event_count;
 	bool signal_event_limit_reached;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	struct rb_root_cached bo_interval_tree;
+#else
 	struct rb_root bo_interval_tree;
+#endif
 
 	/* Information used for memory eviction */
 	void *process_info;
diff -Nuarp amdgpu-17.50-511655.el7.orig/amd/amdkfd/kfd_process.c amdgpu-17.50-511655.el7/amd/amdkfd/kfd_process.c
--- amdgpu-17.50-511655.el7.orig/amd/amdkfd/kfd_process.c	2017-12-02 06:30:56.000000000 +0900
+++ amdgpu-17.50-511655.el7/amd/amdkfd/kfd_process.c	2018-03-02 16:51:39.203194000 +0900
@@ -584,7 +584,11 @@ static struct kfd_process *create_proces
 	if (!process)
 		goto err_alloc_process;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	process->bo_interval_tree = RB_ROOT_CACHED;
+#else
 	process->bo_interval_tree = RB_ROOT;
+#endif
 
 	process->pasid = kfd_pasid_alloc();
 	if (process->pasid == 0)
diff -Nuarp amdgpu-17.50-511655.el7.orig/amd/display/amdgpu_dm/amdgpu_dm.c amdgpu-17.50-511655.el7/amd/display/amdgpu_dm/amdgpu_dm.c
--- amdgpu-17.50-511655.el7.orig/amd/display/amdgpu_dm/amdgpu_dm.c	2017-12-02 06:30:56.000000000 +0900
+++ amdgpu-17.50-511655.el7/amd/display/amdgpu_dm/amdgpu_dm.c	2018-03-02 20:00:47.361099000 +0900
@@ -244,7 +244,11 @@ static void dm_pflip_high_irq(void *inte
 	/* wakeup usersapce */
 	if (amdgpu_crtc->event) {
 		/* Update to correct count/ts if racing with vblank irq */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+		drm_crtc_accurate_vblank_count(&amdgpu_crtc->base);
+#else
 		drm_accurate_vblank_count(&amdgpu_crtc->base);
+#endif
 
 		drm_crtc_send_vblank_event(&amdgpu_crtc->base, amdgpu_crtc->event);
 
@@ -583,11 +587,11 @@ struct amdgpu_dm_connector *amdgpu_dm_fi
 	struct drm_connector *connector;
 	struct drm_crtc *crtc_from_state;
 
-	for_each_connector_in_state(
-		state,
-		connector,
-		conn_state,
-		i) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	for_each_new_connector_in_state(state, connector, conn_state, i) {
+#else
+	for_each_connector_in_state(state, connector, conn_state, i) {
+#endif
 		crtc_from_state =
 			from_state_var ?
 				conn_state->crtc :
@@ -2685,7 +2689,9 @@ static const struct drm_crtc_funcs amdgp
 	.destroy = amdgpu_dm_crtc_destroy,
 	.gamma_set = drm_atomic_helper_legacy_gamma_set,
 	.set_config = drm_atomic_helper_set_config,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 14, 0)
 	.set_property = drm_atomic_helper_crtc_set_property,
+#endif
 #if LINUX_VERSION_CODE < KERNEL_VERSION(4, 11, 0)
 	.page_flip = amdgpu_atomic_helper_page_flip,
 #else
@@ -2955,11 +2961,15 @@ struct drm_connector_state *amdgpu_dm_co
 }
 
 static const struct drm_connector_funcs amdgpu_dm_connector_funcs = {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 14, 0)
 	.dpms = drm_atomic_helper_connector_dpms,
+#endif
 	.reset = amdgpu_dm_connector_funcs_reset,
 	.detect = amdgpu_dm_connector_detect,
 	.fill_modes = drm_helper_probe_single_connector_modes,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 14, 0)
 	.set_property = drm_atomic_helper_connector_set_property,
+#endif
 	.destroy = amdgpu_dm_connector_destroy,
 	.atomic_duplicate_state = amdgpu_dm_connector_atomic_duplicate_state,
 	.atomic_destroy_state = drm_atomic_helper_connector_destroy_state,
@@ -2977,7 +2987,11 @@ static struct drm_encoder *best_encoder(
 
 	/* pick the encoder ids */
 	if (enc_id) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+		obj = drm_mode_object_find(connector->dev, NULL, enc_id, DRM_MODE_OBJECT_ENCODER);
+#else
 		obj = drm_mode_object_find(connector->dev, enc_id, DRM_MODE_OBJECT_ENCODER);
+#endif
 		if (!obj) {
 			DRM_ERROR("Couldn't find a matching encoder for our connector\n");
 			return NULL;
@@ -3222,7 +3236,9 @@ static const struct drm_plane_funcs dm_p
 	.update_plane	= drm_atomic_helper_update_plane,
 	.disable_plane	= drm_atomic_helper_disable_plane,
 	.destroy	= drm_plane_cleanup,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 14, 0)
 	.set_property	= drm_atomic_helper_plane_set_property,
+#endif
 	.reset = dm_drm_plane_reset,
 	.atomic_duplicate_state = dm_drm_plane_duplicate_state,
 	.atomic_destroy_state = dm_drm_plane_destroy_state,
@@ -4236,6 +4252,111 @@ static void amdgpu_dm_do_flip(
 	spin_unlock_irqrestore(&crtc->dev->event_lock, flags);
 }
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+static void amdgpu_dm_commit_planes(struct drm_atomic_state *state,
+				    struct drm_device *dev,
+				    struct amdgpu_display_manager *dm,
+				    struct drm_crtc *pcrtc,
+				    bool *wait_for_vblank)
+{
+	uint32_t i;
+	struct drm_plane *plane;
+	struct drm_plane_state *old_plane_state, *new_plane_state;
+	struct dc_stream_state *dc_stream_attach;
+	struct dc_plane_state *plane_states_constructed[MAX_SURFACES];
+	struct amdgpu_crtc *acrtc_attach = to_amdgpu_crtc(pcrtc);
+	struct drm_crtc_state *new_pcrtc_state =
+			drm_atomic_get_new_crtc_state(state, pcrtc);
+	struct dm_crtc_state *acrtc_state = to_dm_crtc_state(new_pcrtc_state);
+	struct dm_atomic_state *dm_state = to_dm_atomic_state(state);
+	int planes_count = 0;
+	unsigned long flags;
+
+	/* update planes when needed */
+	for_each_oldnew_plane_in_state(state, plane, old_plane_state, new_plane_state, i) {
+		struct drm_crtc *crtc = new_plane_state->crtc;
+		struct drm_crtc_state *new_crtc_state;
+		struct drm_framebuffer *fb = new_plane_state->fb;
+		bool pflip_needed;
+		struct dm_plane_state *dm_new_plane_state = to_dm_plane_state(new_plane_state);
+
+		if (plane->type == DRM_PLANE_TYPE_CURSOR) {
+			handle_cursor_update(plane, old_plane_state);
+			continue;
+		}
+
+		if (!fb || !crtc || pcrtc != crtc)
+			continue;
+
+		new_crtc_state = drm_atomic_get_new_crtc_state(state, crtc);
+		if (!new_crtc_state->active)
+			continue;
+
+		pflip_needed = !state->allow_modeset;
+
+		spin_lock_irqsave(&crtc->dev->event_lock, flags);
+		if (acrtc_attach->pflip_status != AMDGPU_FLIP_NONE) {
+			DRM_ERROR("%s: acrtc %d, already busy\n",
+				  __func__,
+				  acrtc_attach->crtc_id);
+			/* In commit tail framework this cannot happen */
+			WARN_ON(1);
+		}
+		spin_unlock_irqrestore(&crtc->dev->event_lock, flags);
+
+		if (!pflip_needed) {
+			WARN_ON(!dm_new_plane_state->dc_state);
+
+			plane_states_constructed[planes_count] = dm_new_plane_state->dc_state;
+
+			dc_stream_attach = acrtc_state->stream;
+			planes_count++;
+
+		} else if (new_crtc_state->planes_changed) {
+			/* Assume even ONE crtc with immediate flip means
+			 * entire can't wait for VBLANK
+			 * TODO Check if it's correct
+			 */
+			*wait_for_vblank =
+					new_pcrtc_state->pageflip_flags & DRM_MODE_PAGE_FLIP_ASYNC ?
+				false : true;
+
+			/* TODO: Needs rework for multiplane flip */
+			if (plane->type == DRM_PLANE_TYPE_PRIMARY)
+				drm_crtc_vblank_get(crtc);
+
+			amdgpu_dm_do_flip(
+				crtc,
+				fb,
+				drm_crtc_vblank_count(crtc) + *wait_for_vblank);
+//				dm_state->context);
+		}
+
+	}
+
+	if (planes_count) {
+		unsigned long flags;
+
+		if (new_pcrtc_state->event) {
+
+			drm_crtc_vblank_get(pcrtc);
+
+			spin_lock_irqsave(&pcrtc->dev->event_lock, flags);
+			prepare_flip_isr(acrtc_attach);
+			spin_unlock_irqrestore(&pcrtc->dev->event_lock, flags);
+		}
+
+		if (false == dc_commit_planes_to_stream(dm->dc,
+							plane_states_constructed,
+							planes_count,
+							dc_stream_attach))
+//							dm_state->context))
+			dm_error("%s: Failed to attach plane!\n", __func__);
+	} else {
+		/*TODO BUG Here should go disable planes on CRTC. */
+	}
+}
+#else
 static void amdgpu_dm_commit_planes(struct drm_atomic_state *state,
 			struct drm_device *dev,
 			struct amdgpu_display_manager *dm,
@@ -4244,7 +4365,7 @@ static void amdgpu_dm_commit_planes(stru
 {
 	uint32_t i;
 	struct drm_plane *plane;
-	struct drm_plane_state *old_plane_state;
+	struct drm_plane_state *old_plane_state/*, *new_plane_state*/;
 	struct dc_stream_state *dc_stream_attach;
 	struct dc_plane_state *plane_states_constructed[MAX_SURFACES];
 	struct amdgpu_crtc *acrtc_attach = to_amdgpu_crtc(pcrtc);
@@ -4338,6 +4459,7 @@ static void amdgpu_dm_commit_planes(stru
 		/*TODO BUG Here should go disable planes on CRTC. */
 	}
 }
+#endif
 
 
 int amdgpu_dm_atomic_commit(
@@ -4357,7 +4479,11 @@ int amdgpu_dm_atomic_commit(
 	 * it will update crtc->dm_crtc_state->stream pointer which is used in
 	 * the ISRs.
 	 */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	for_each_new_crtc_in_state(state, crtc, new_state, i) {
+#else
 	for_each_crtc_in_state(state, crtc, new_state, i) {
+#endif
 		struct dm_crtc_state *old_acrtc_state = to_dm_crtc_state(crtc->state);
 		struct amdgpu_crtc *acrtc = to_amdgpu_crtc(crtc);
 
@@ -4370,6 +4496,271 @@ int amdgpu_dm_atomic_commit(
 	/*TODO Handle EINTR, reenable IRQ*/
 }
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+void amdgpu_dm_atomic_commit_tail(struct drm_atomic_state *state)
+{
+	struct drm_device *dev = state->dev;
+	struct amdgpu_device *adev = dev->dev_private;
+	struct amdgpu_display_manager *dm = &adev->dm;
+	struct dm_atomic_state *dm_state;
+	uint32_t i, j;
+	struct drm_crtc *crtc;
+	struct drm_crtc_state *old_crtc_state, *new_crtc_state;
+	unsigned long flags;
+	bool wait_for_vblank = true;
+	struct drm_connector *connector;
+	struct drm_connector_state *old_con_state, *new_con_state;
+	struct dm_crtc_state *dm_old_crtc_state, *dm_new_crtc_state;
+
+	drm_atomic_helper_update_legacy_modeset_state(dev, state);
+
+	dm_state = to_dm_atomic_state(state);
+
+	/* update changed items */
+	for_each_oldnew_crtc_in_state(state, crtc, old_crtc_state, new_crtc_state, i) {
+		struct amdgpu_crtc *acrtc = to_amdgpu_crtc(crtc);
+
+		dm_new_crtc_state = to_dm_crtc_state(new_crtc_state);
+		dm_old_crtc_state = to_dm_crtc_state(old_crtc_state);
+
+		DRM_DEBUG_DRIVER(
+			"amdgpu_crtc id:%d crtc_state_flags: enable:%d, active:%d, "
+			"planes_changed:%d, mode_changed:%d,active_changed:%d,"
+			"connectors_changed:%d\n",
+			acrtc->crtc_id,
+			new_crtc_state->enable,
+			new_crtc_state->active,
+			new_crtc_state->planes_changed,
+			new_crtc_state->mode_changed,
+			new_crtc_state->active_changed,
+			new_crtc_state->connectors_changed);
+
+		/* Copy all transient state flags into dc state */
+//		if (dm_new_crtc_state->stream) {
+//			amdgpu_dm_crtc_copy_transient_flags(&dm_new_crtc_state->base,
+//							    dm_new_crtc_state->stream);
+//		}
+
+		/* handles headless hotplug case, updating new_state and
+		 * aconnector as needed
+		 */
+
+		if (modeset_required(new_crtc_state, dm_new_crtc_state->stream, dm_old_crtc_state->stream)) {
+
+			DRM_DEBUG_DRIVER("Atomic commit: SET crtc id %d: [%p]\n", acrtc->crtc_id, acrtc);
+
+			if (!dm_new_crtc_state->stream) {
+				/*
+				 * this could happen because of issues with
+				 * userspace notifications delivery.
+				 * In this case userspace tries to set mode on
+				 * display which is disconnect in fact.
+				 * dc_sink in NULL in this case on aconnector.
+				 * We expect reset mode will come soon.
+				 *
+				 * This can also happen when unplug is done
+				 * during resume sequence ended
+				 *
+				 * In this case, we want to pretend we still
+				 * have a sink to keep the pipe running so that
+				 * hw state is consistent with the sw state
+				 */
+				DRM_DEBUG_DRIVER("%s: Failed to create new stream for crtc %d\n",
+						__func__, acrtc->base.base.id);
+				continue;
+			}
+
+			if (dm_old_crtc_state->stream)
+				remove_stream(adev, acrtc, dm_old_crtc_state->stream);
+
+			acrtc->enabled = true;
+			acrtc->hw_mode = new_crtc_state->mode;
+			crtc->hwmode = new_crtc_state->mode;
+		} else if (modereset_required(new_crtc_state)) {
+			DRM_DEBUG_DRIVER("Atomic commit: RESET. crtc id %d:[%p]\n", acrtc->crtc_id, acrtc);
+
+			/* i.e. reset mode */
+			if (dm_old_crtc_state->stream)
+				remove_stream(adev, acrtc, dm_old_crtc_state->stream);
+		}
+	} /* for_each_crtc_in_state() */
+
+	/*
+	 * Add streams after required streams from new and replaced streams
+	 * are removed from freesync module
+	 */
+	if (adev->dm.freesync_module) {
+		for_each_oldnew_crtc_in_state(state, crtc, old_crtc_state,
+					      new_crtc_state, i) {
+			struct amdgpu_dm_connector *aconnector = NULL;
+			struct dm_connector_state *dm_new_con_state = NULL;
+			struct amdgpu_crtc *acrtc = NULL;
+			bool modeset_needed;
+
+			dm_new_crtc_state = to_dm_crtc_state(new_crtc_state);
+			dm_old_crtc_state = to_dm_crtc_state(old_crtc_state);
+			modeset_needed = modeset_required(
+					new_crtc_state,
+					dm_new_crtc_state->stream,
+					dm_old_crtc_state->stream);
+			/* We add stream to freesync if:
+			 * 1. Said stream is not null, and
+			 * 2. A modeset is requested. This means that the
+			 *    stream was removed previously, and needs to be
+			 *    replaced.
+			 */
+			if (dm_new_crtc_state->stream == NULL ||
+					!modeset_needed)
+				continue;
+
+			acrtc = to_amdgpu_crtc(crtc);
+
+			aconnector =
+				amdgpu_dm_find_first_crct_matching_connector(
+					state, crtc, false);
+			if (!aconnector) {
+				DRM_DEBUG_DRIVER("Atomic commit: Failed to "
+						 "find connector for acrtc "
+						 "id:%d skipping freesync "
+						 "init\n",
+						 acrtc->crtc_id);
+				continue;
+			}
+
+			mod_freesync_add_stream(adev->dm.freesync_module,
+						dm_new_crtc_state->stream,
+						&aconnector->caps);
+			new_con_state = drm_atomic_get_new_connector_state(
+					state, &aconnector->base);
+			dm_new_con_state = to_dm_connector_state(new_con_state);
+
+			/*mod_freesync_set_user_enable(adev->dm.freesync_module,
+						     &dm_new_crtc_state->stream,
+						     1,
+						     &dm_new_con_state->user_enable);*/
+		}
+	}
+
+	if (dm_state->context) {
+//		dm_enable_per_frame_crtc_master_sync(dm_state->context);
+//		WARN_ON(!dc_commit_state(dm->dc, dm_state->context));
+		WARN_ON(!dc_commit_context(dm->dc, dm_state->context));
+	}
+
+	for_each_new_crtc_in_state(state, crtc, new_crtc_state, i) {
+		struct amdgpu_crtc *acrtc = to_amdgpu_crtc(crtc);
+
+		dm_new_crtc_state = to_dm_crtc_state(new_crtc_state);
+
+		if (dm_new_crtc_state->stream != NULL) {
+			const struct dc_stream_status *status =
+					dc_stream_get_status(dm_new_crtc_state->stream);
+
+			if (!status)
+				DC_ERR("got no status for stream %p on acrtc%p\n", dm_new_crtc_state->stream, acrtc);
+			else
+				acrtc->otg_inst = status->primary_otg_inst;
+		}
+	}
+
+	/* Handle scaling and underscan changes*/
+	for_each_oldnew_connector_in_state(state, connector, old_con_state, new_con_state, i) {
+		struct dm_connector_state *dm_new_con_state = to_dm_connector_state(new_con_state);
+		struct dm_connector_state *dm_old_con_state = to_dm_connector_state(old_con_state);
+		struct amdgpu_crtc *acrtc = to_amdgpu_crtc(dm_new_con_state->base.crtc);
+		struct dc_stream_status *status = NULL;
+
+		if (acrtc)
+			new_crtc_state = drm_atomic_get_new_crtc_state(state, &acrtc->base);
+
+		/* Skip any modesets/resets */
+		if (!acrtc || drm_atomic_crtc_needs_modeset(new_crtc_state))
+			continue;
+
+		/* Skip any thing not scale or underscan changes */
+		if (!is_scaling_state_different(dm_new_con_state, dm_old_con_state))
+			continue;
+
+		dm_new_crtc_state = to_dm_crtc_state(new_crtc_state);
+
+		update_stream_scaling_settings(&dm_new_con_state->base.crtc->mode,
+				dm_new_con_state, (struct dc_stream_state *)dm_new_crtc_state->stream);
+
+		if (!dm_new_crtc_state->stream)
+			continue;
+
+		status = dc_stream_get_status(dm_new_crtc_state->stream);
+		WARN_ON(!status);
+		WARN_ON(!status->plane_count);
+
+		/*TODO How it works with MPO ?*/
+		if (!dc_commit_planes_to_stream(
+				dm->dc,
+				status->plane_states,
+				status->plane_count,
+				dm_new_crtc_state->stream/*,
+				dm_state->context*/))
+			dm_error("%s: Failed to update stream scaling!\n", __func__);
+	}
+
+	for_each_oldnew_crtc_in_state(state, crtc, old_crtc_state,
+			new_crtc_state, i) {
+		/*
+		 * loop to enable interrupts on newly arrived crtc
+		 */
+		struct amdgpu_crtc *acrtc = to_amdgpu_crtc(crtc);
+		bool modeset_needed;
+
+		dm_new_crtc_state = to_dm_crtc_state(new_crtc_state);
+		dm_old_crtc_state = to_dm_crtc_state(old_crtc_state);
+		modeset_needed = modeset_required(
+				new_crtc_state,
+				dm_new_crtc_state->stream,
+				dm_old_crtc_state->stream);
+
+		if (dm_new_crtc_state->stream == NULL || !modeset_needed)
+			continue;
+
+		if (adev->dm.freesync_module)
+			mod_freesync_notify_mode_change(
+				adev->dm.freesync_module,
+				&dm_new_crtc_state->stream, 1);
+
+		manage_dm_interrupts(adev, acrtc, true);
+	}
+
+	/* update planes when needed per crtc*/
+	for_each_new_crtc_in_state(state, crtc, new_crtc_state, j) {
+		dm_new_crtc_state = to_dm_crtc_state(new_crtc_state);
+
+		if (dm_new_crtc_state->stream)
+			amdgpu_dm_commit_planes(state, dev, dm, crtc, &wait_for_vblank);
+	}
+
+
+	/*
+	 * send vblank event on all events not handled in flip and
+	 * mark consumed event for drm_atomic_helper_commit_hw_done
+	 */
+	spin_lock_irqsave(&adev->ddev->event_lock, flags);
+	for_each_new_crtc_in_state(state, crtc, new_crtc_state, i) {
+
+		if (new_crtc_state->event)
+			drm_send_event_locked(dev, &new_crtc_state->event->base);
+
+		new_crtc_state->event = NULL;
+	}
+	spin_unlock_irqrestore(&adev->ddev->event_lock, flags);
+
+	/* Signal HW programming completion */
+	drm_atomic_helper_commit_hw_done(state);
+
+	if (wait_for_vblank)
+		drm_atomic_helper_wait_for_flip_done(dev, state);
+
+	drm_atomic_helper_cleanup_planes(dev, state);
+}
+#else
 void amdgpu_dm_atomic_commit_tail(
 	struct drm_atomic_state *state)
 {
@@ -4394,7 +4785,11 @@ void amdgpu_dm_atomic_commit_tail(
 	dm_state = to_dm_atomic_state(state);
 
 	/* update changed items */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	for_each_old_crtc_in_state(state, crtc, old_crtc_state/*, new_crtc_state*/, i) {
+#else
 	for_each_crtc_in_state(state, crtc, old_crtc_state, i) {
+#endif
 		struct amdgpu_crtc *acrtc = to_amdgpu_crtc(crtc);
 		struct drm_crtc_state *new_state = crtc->state;
 
@@ -4523,7 +4918,11 @@ void amdgpu_dm_atomic_commit_tail(
 	}
 
 	/* Handle scaling and undersacn changes*/
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	for_each_old_connector_in_state(state, connector, old_conn_state/*, new_con_state*/, i) {
+#else
 	for_each_connector_in_state(state, connector, old_conn_state, i) {
+#endif
 		struct amdgpu_dm_connector *aconnector = to_amdgpu_dm_connector(connector);
 		struct dm_connector_state *con_new_state =
 				to_dm_connector_state(aconnector->base.state);
@@ -4577,7 +4976,17 @@ void amdgpu_dm_atomic_commit_tail(
 	}
 
 	/* update planes when needed per crtc*/
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	for_each_old_crtc_in_state(state, pcrtc, old_crtc_state, j) {
+	/*for_each_new_crtc_in_state(state, pcrtc, new_crtc_state, j) {
+		new_acrtc_state = to_dm_crtc_state(new_crtc_state);
+
+		if (new_acrtc_state->stream)
+			amdgpu_dm_commit_planes(state, dev, dm, pcrtc, &wait_for_vblank);
+	}*/
+#else
 	for_each_crtc_in_state(state, pcrtc, old_crtc_state, j) {
+#endif
 		new_acrtc_state = to_dm_crtc_state(pcrtc->state);
 
 		if (new_acrtc_state->stream)
@@ -4590,7 +4999,11 @@ void amdgpu_dm_atomic_commit_tail(
 	 * mark consumed event for drm_atomic_helper_commit_hw_done
 	 */
 	spin_lock_irqsave(&adev->ddev->event_lock, flags);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	for_each_old_crtc_in_state(state, crtc, old_crtc_state, i) {
+#else
 	for_each_crtc_in_state(state, crtc, old_crtc_state, i) {
+#endif
 		struct amdgpu_crtc *acrtc = to_amdgpu_crtc(crtc);
 
 		if (acrtc->base.state->event)
@@ -4608,6 +5021,7 @@ void amdgpu_dm_atomic_commit_tail(
 
 	drm_atomic_helper_cleanup_planes(dev, state);
 }
+#endif
 
 
 static int dm_force_atomic_commit(struct drm_connector *connector)
@@ -4868,7 +5282,11 @@ int amdgpu_dm_atomic_check(struct drm_de
 
 	/*TODO Move this code into dm_crtc_atomic_check once we get rid of dc_validation_set */
 	/* update changed items */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	for_each_new_crtc_in_state(state, crtc, crtc_state, i) {
+#else
 	for_each_crtc_in_state(state, crtc, crtc_state, i) {
+#endif
 		struct amdgpu_crtc *acrtc = NULL;
 		struct amdgpu_dm_connector *aconnector = NULL;
 		struct dc_stream_state *new_stream = NULL;
@@ -4988,7 +5406,11 @@ int amdgpu_dm_atomic_check(struct drm_de
 	 * new stream into context w\o causing full reset. Need to
 	 * decide how to handle.
 	 */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	for_each_new_connector_in_state(state, connector, conn_state, i) {
+#else
 	for_each_connector_in_state(state, connector, conn_state, i) {
+#endif
 		struct amdgpu_dm_connector *aconnector = to_amdgpu_dm_connector(connector);
 		struct dm_connector_state *con_old_state =
 				to_dm_connector_state(aconnector->base.state);
@@ -5007,10 +5429,18 @@ int amdgpu_dm_atomic_check(struct drm_de
 		lock_and_validation_needed = true;
 	}
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	for_each_new_crtc_in_state(state, crtc, crtc_state, i) {
+#else
 	for_each_crtc_in_state(state, crtc, crtc_state, i) {
+#endif
 		new_acrtc_state = to_dm_crtc_state(crtc_state);
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+		for_each_new_plane_in_state(state, plane, plane_state, j) {
+#else
 		for_each_plane_in_state(state, plane, plane_state, j) {
+#endif
 			struct drm_crtc *plane_crtc = plane_state->crtc;
 			struct drm_framebuffer *fb = plane_state->fb;
 			bool pflip_needed;
diff -Nuarp amdgpu-17.50-511655.el7.orig/amd/display/amdgpu_dm/amdgpu_dm_mst_types.c amdgpu-17.50-511655.el7/amd/display/amdgpu_dm/amdgpu_dm_mst_types.c
--- amdgpu-17.50-511655.el7.orig/amd/display/amdgpu_dm/amdgpu_dm_mst_types.c	2017-12-02 06:30:56.000000000 +0900
+++ amdgpu-17.50-511655.el7/amd/display/amdgpu_dm/amdgpu_dm_mst_types.c	2018-03-02 16:28:09.313128000 +0900
@@ -164,12 +164,16 @@ dm_dp_mst_connector_destroy(struct drm_c
 }
 
 static const struct drm_connector_funcs dm_dp_mst_connector_funcs = {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 14, 0)
 	.dpms = drm_atomic_helper_connector_dpms,
+#endif
 	.detect = dm_dp_mst_detect,
 	.fill_modes = drm_helper_probe_single_connector_modes,
 	.destroy = dm_dp_mst_connector_destroy,
 	.reset = amdgpu_dm_connector_funcs_reset,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 14, 0)
 	.set_property = drm_atomic_helper_connector_set_property,
+#endif
 	.atomic_duplicate_state = amdgpu_dm_connector_atomic_duplicate_state,
 	.atomic_destroy_state = drm_atomic_helper_connector_destroy_state,
 	.atomic_set_property = amdgpu_dm_connector_atomic_set_property,
diff -Nuarp amdgpu-17.50-511655.el7.orig/amd/display/dc/bios/bios_parser2.c amdgpu-17.50-511655.el7/amd/display/dc/bios/bios_parser2.c
--- amdgpu-17.50-511655.el7.orig/amd/display/dc/bios/bios_parser2.c	2017-12-02 06:30:56.000000000 +0900
+++ amdgpu-17.50-511655.el7/amd/display/dc/bios/bios_parser2.c	2018-03-02 20:10:30.163633000 +0900
@@ -1326,13 +1326,13 @@ static enum bp_result get_embedded_panel
 	info->lcd_timing.misc_info.VERTICAL_CUT_OFF = 0;
 
 	info->lcd_timing.misc_info.H_REPLICATION_BY2 =
-		lvds->lcd_timing.miscinfo & ATOM_H_REPLICATIONBY2;
+		!!(lvds->lcd_timing.miscinfo & ATOM_H_REPLICATIONBY2);
 	info->lcd_timing.misc_info.V_REPLICATION_BY2 =
-		lvds->lcd_timing.miscinfo & ATOM_V_REPLICATIONBY2;
+		!!(lvds->lcd_timing.miscinfo & ATOM_V_REPLICATIONBY2);
 	info->lcd_timing.misc_info.COMPOSITE_SYNC =
-		lvds->lcd_timing.miscinfo & ATOM_COMPOSITESYNC;
+		!!(lvds->lcd_timing.miscinfo & ATOM_COMPOSITESYNC);
 	info->lcd_timing.misc_info.INTERLACE =
-		lvds->lcd_timing.miscinfo & ATOM_INTERLACE;
+		!!(lvds->lcd_timing.miscinfo & ATOM_INTERLACE);
 
 	/* not provided by VBIOS*/
 	info->lcd_timing.misc_info.DOUBLE_CLOCK = 0;
diff -Nuarp amdgpu-17.50-511655.el7.orig/amd/scheduler/gpu_scheduler.c amdgpu-17.50-511655.el7/amd/scheduler/gpu_scheduler.c
--- amdgpu-17.50-511655.el7.orig/amd/scheduler/gpu_scheduler.c	2017-12-02 06:30:56.000000000 +0900
+++ amdgpu-17.50-511655.el7/amd/scheduler/gpu_scheduler.c	2018-03-02 15:54:10.705841000 +0900
@@ -189,7 +189,11 @@ static bool amd_sched_entity_is_ready(st
 	if (kfifo_is_empty(&entity->job_queue))
 		return false;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	if (READ_ONCE(entity->dependency))
+#else
 	if (ACCESS_ONCE(entity->dependency))
+#endif
 		return false;
 
 	return true;
diff -Nuarp amdgpu-17.50-511655.el7.orig/include/drm/spsc_queue.h amdgpu-17.50-511655.el7/include/drm/spsc_queue.h
--- amdgpu-17.50-511655.el7.orig/include/drm/spsc_queue.h	1970-01-01 09:00:00.000000000 +0900
+++ amdgpu-17.50-511655.el7/include/drm/spsc_queue.h	2018-02-08 05:43:08.000000000 +0900
@@ -0,0 +1,122 @@
+/*
+ * Copyright 2017 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+#ifndef DRM_SCHEDULER_SPSC_QUEUE_H_
+#define DRM_SCHEDULER_SPSC_QUEUE_H_
+
+#include <linux/atomic.h>
+#include <linux/preempt.h>
+
+/** SPSC lockless queue */
+
+struct spsc_node {
+
+	/* Stores spsc_node* */
+	struct spsc_node *next;
+};
+
+struct spsc_queue {
+
+	 struct spsc_node *head;
+
+	/* atomic pointer to struct spsc_node* */
+	atomic_long_t tail;
+
+	atomic_t job_count;
+};
+
+static inline void spsc_queue_init(struct spsc_queue *queue)
+{
+	queue->head = NULL;
+	atomic_long_set(&queue->tail, (long)&queue->head);
+	atomic_set(&queue->job_count, 0);
+}
+
+static inline struct spsc_node *spsc_queue_peek(struct spsc_queue *queue)
+{
+	return queue->head;
+}
+
+static inline int spsc_queue_count(struct spsc_queue *queue)
+{
+	return atomic_read(&queue->job_count);
+}
+
+static inline bool spsc_queue_push(struct spsc_queue *queue, struct spsc_node *node)
+{
+	struct spsc_node **tail;
+
+	node->next = NULL;
+
+	preempt_disable();
+
+	tail = (struct spsc_node **)atomic_long_xchg(&queue->tail, (long)&node->next);
+	WRITE_ONCE(*tail, node);
+	atomic_inc(&queue->job_count);
+
+	/*
+	 * In case of first element verify new node will be visible to the consumer
+	 * thread when we ping the kernel thread that there is new work to do.
+	 */
+	smp_wmb();
+
+	preempt_enable();
+
+	return tail == &queue->head;
+}
+
+
+static inline struct spsc_node *spsc_queue_pop(struct spsc_queue *queue)
+{
+	struct spsc_node *next, *node;
+
+	/* Verify reading from memory and not the cache */
+	smp_rmb();
+
+	node = READ_ONCE(queue->head);
+
+	if (!node)
+		return NULL;
+
+	next = READ_ONCE(node->next);
+	WRITE_ONCE(queue->head, next);
+
+	if (unlikely(!next)) {
+		/* slowpath for the last element in the queue */
+
+		if (atomic_long_cmpxchg(&queue->tail,
+				(long)&node->next, (long) &queue->head) != (long)&node->next) {
+			/* Updating tail failed wait for new next to appear */
+			do {
+				smp_rmb();
+			} while (unlikely(!(queue->head = READ_ONCE(node->next))));
+		}
+	}
+
+	atomic_dec(&queue->job_count);
+	return node;
+}
+
+
+
+#endif /* DRM_SCHEDULER_SPSC_QUEUE_H_ */
diff -Nuarp amdgpu-17.50-511655.el7.orig/include/kcl/kcl_drm.h amdgpu-17.50-511655.el7/include/kcl/kcl_drm.h
--- amdgpu-17.50-511655.el7.orig/include/kcl/kcl_drm.h	2017-12-02 06:30:56.000000000 +0900
+++ amdgpu-17.50-511655.el7/include/kcl/kcl_drm.h	2018-03-02 21:57:38.635506000 +0900
@@ -277,7 +277,10 @@ static inline int kcl_drm_universal_plan
 			     enum drm_plane_type type,
 			     const char *name, ...)
 {
-#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 5, 0) || \
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+		return drm_universal_plane_init(dev, plane, possible_crtcs, funcs,
+				 formats, format_count, 0, type, name);
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(4, 5, 0) || \
 		defined(OS_NAME_RHEL_7_3) || \
 		defined(OS_NAME_RHEL_7_4)
 		return drm_universal_plane_init(dev, plane, possible_crtcs, funcs,
@@ -330,7 +333,11 @@ static inline int
 kcl_drm_calc_vbltimestamp_from_scanoutpos(struct drm_device *dev,
 					  unsigned int pipe,
 					  int *max_error,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 14, 0)
 					  struct timeval *vblank_time,
+#else
+					  ktime_t *vblank_time,
+#endif
 #if LINUX_VERSION_CODE < KERNEL_VERSION(4, 13, 0)
 					  unsigned flags,
 #else
diff -Nuarp amdgpu-17.50-511655.el7.orig/include/rename_symbol.h amdgpu-17.50-511655.el7/include/rename_symbol.h
--- amdgpu-17.50-511655.el7.orig/include/rename_symbol.h	1970-01-01 09:00:00.000000000 +0900
+++ amdgpu-17.50-511655.el7/include/rename_symbol.h	2018-03-02 21:23:28.696444000 +0900
@@ -0,0 +1,188 @@
+#define ttm_agp_tt_create amdttm_agp_tt_create //ttm/ttm_agp_backend.c:EXPORT_SYMBOL(ttm_agp_tt_create);
+#define ttm_agp_tt_populate amdttm_agp_tt_populate //ttm/ttm_agp_backend.c:EXPORT_SYMBOL(ttm_agp_tt_populate);
+#define ttm_agp_tt_unpopulate amdttm_agp_tt_unpopulate //ttm/ttm_agp_backend.c:EXPORT_SYMBOL(ttm_agp_tt_unpopulate);
+#define ttm_bo_acc_size amdttm_bo_acc_size //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_acc_size);
+#define ttm_bo_add_to_lru amdttm_bo_add_to_lru //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_add_to_lru);
+#define ttm_bo_clean_mm amdttm_bo_clean_mm //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_clean_mm);
+#define ttm_bo_create amdttm_bo_create //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_create);
+#define ttm_bo_del_sub_from_lru amdttm_bo_del_sub_from_lru //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_del_sub_from_lru);
+#define ttm_bo_device_init amdttm_bo_device_init //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_device_init);
+#define ttm_bo_device_release amdttm_bo_device_release //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_device_release);
+#define ttm_bo_dma_acc_size amdttm_bo_dma_acc_size //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_dma_acc_size);
+#define ttm_bo_evict_mm amdttm_bo_evict_mm //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_evict_mm);
+#define ttm_bo_eviction_valuable amdttm_bo_eviction_valuable //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_eviction_valuable);
+#define ttm_bo_global_init amdttm_bo_global_init //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_global_init);
+#define ttm_bo_global_release amdttm_bo_global_release //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_global_release);
+#define ttm_bo_init amdttm_bo_init //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_init);
+#define ttm_bo_init_mm amdttm_bo_init_mm //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_init_mm);
+#define ttm_bo_init_reserved amdttm_bo_init_reserved //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_init_reserved);
+#define ttm_bo_lock_delayed_workqueue amdttm_bo_lock_delayed_workqueue //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_lock_delayed_workqueue);
+#define ttm_bo_mem_compat amdttm_bo_mem_compat //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_mem_compat);
+#define ttm_bo_mem_put amdttm_bo_mem_put //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_mem_put);
+#define ttm_bo_mem_space amdttm_bo_mem_space //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_mem_space);
+#define ttm_bo_move_to_lru_tail amdttm_bo_move_to_lru_tail //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_move_to_lru_tail);
+#define ttm_bo_swapout_all amdttm_bo_swapout_all //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_swapout_all);
+#define ttm_bo_synccpu_write_grab amdttm_bo_synccpu_write_grab //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_synccpu_write_grab);
+#define ttm_bo_synccpu_write_release amdttm_bo_synccpu_write_release //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_synccpu_write_release);
+#define ttm_bo_unlock_delayed_workqueue amdttm_bo_unlock_delayed_workqueue //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_unlock_delayed_workqueue);
+#define ttm_bo_unmap_virtual amdttm_bo_unmap_virtual //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_unmap_virtual);
+#define ttm_bo_unref amdttm_bo_unref //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_unref);
+#define ttm_bo_validate amdttm_bo_validate //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_validate);
+#define ttm_bo_wait amdttm_bo_wait //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_wait);
+#define ttm_bo_manager_func amdttm_bo_manager_func //ttm/ttm_bo_manager.c:EXPORT_SYMBOL(ttm_bo_manager_func);
+#define ttm_bo_kmap amdttm_bo_kmap //ttm/ttm_bo_util.c:EXPORT_SYMBOL(ttm_bo_kmap);
+#define ttm_bo_kunmap amdttm_bo_kunmap //ttm/ttm_bo_util.c:EXPORT_SYMBOL(ttm_bo_kunmap);
+#define ttm_bo_move_accel_cleanup amdttm_bo_move_accel_cleanup //ttm/ttm_bo_util.c:EXPORT_SYMBOL(ttm_bo_move_accel_cleanup);
+#define ttm_bo_move_memcpy amdttm_bo_move_memcpy //ttm/ttm_bo_util.c:EXPORT_SYMBOL(ttm_bo_move_memcpy);
+#define ttm_bo_move_ttm amdttm_bo_move_ttm //ttm/ttm_bo_util.c:EXPORT_SYMBOL(ttm_bo_move_ttm);
+#define ttm_bo_pipeline_move amdttm_bo_pipeline_move //ttm/ttm_bo_util.c:EXPORT_SYMBOL(ttm_bo_pipeline_move);
+#define ttm_io_prot amdttm_io_prot //ttm/ttm_bo_util.c:EXPORT_SYMBOL(ttm_io_prot);
+#define ttm_mem_io_free amdttm_mem_io_free //ttm/ttm_bo_util.c:EXPORT_SYMBOL(ttm_mem_io_free);
+#define ttm_mem_io_lock amdttm_mem_io_lock //ttm/ttm_bo_util.c:EXPORT_SYMBOL(ttm_mem_io_lock);
+#define ttm_mem_io_reserve amdttm_mem_io_reserve //ttm/ttm_bo_util.c:EXPORT_SYMBOL(ttm_mem_io_reserve);
+#define ttm_mem_io_unlock amdttm_mem_io_unlock //ttm/ttm_bo_util.c:EXPORT_SYMBOL(ttm_mem_io_unlock);
+#define ttm_bo_default_io_mem_pfn amdttm_bo_default_io_mem_pfn //ttm/ttm_bo_vm.c:EXPORT_SYMBOL(ttm_bo_default_io_mem_pfn);
+#define ttm_bo_mmap amdttm_bo_mmap //ttm/ttm_bo_vm.c:EXPORT_SYMBOL(ttm_bo_mmap);
+#define ttm_fbdev_mmap amdttm_fbdev_mmap //ttm/ttm_bo_vm.c:EXPORT_SYMBOL(ttm_fbdev_mmap);
+#define ttm_trace_dma_map amdttm_trace_dma_map //ttm/ttm_debug.c:EXPORT_SYMBOL(ttm_trace_dma_map);
+#define ttm_trace_dma_unmap amdttm_trace_dma_unmap //ttm/ttm_debug.c:EXPORT_SYMBOL(ttm_trace_dma_unmap);
+#define ttm_eu_backoff_reservation amdttm_eu_backoff_reservation //ttm/ttm_execbuf_util.c:EXPORT_SYMBOL(ttm_eu_backoff_reservation);
+#define ttm_eu_fence_buffer_objects amdttm_eu_fence_buffer_objects //ttm/ttm_execbuf_util.c:EXPORT_SYMBOL(ttm_eu_fence_buffer_objects);
+#define ttm_eu_reserve_buffers amdttm_eu_reserve_buffers //ttm/ttm_execbuf_util.c:EXPORT_SYMBOL(ttm_eu_reserve_buffers);
+#define ttm_lock_init amdttm_lock_init //ttm/ttm_lock.c:EXPORT_SYMBOL(ttm_lock_init);
+#define ttm_read_lock amdttm_read_lock //ttm/ttm_lock.c:EXPORT_SYMBOL(ttm_read_lock);
+#define ttm_read_unlock amdttm_read_unlock //ttm/ttm_lock.c:EXPORT_SYMBOL(ttm_read_unlock);
+#define ttm_suspend_lock amdttm_suspend_lock //ttm/ttm_lock.c:EXPORT_SYMBOL(ttm_suspend_lock);
+#define ttm_suspend_unlock amdttm_suspend_unlock //ttm/ttm_lock.c:EXPORT_SYMBOL(ttm_suspend_unlock);
+#define ttm_vt_lock amdttm_vt_lock //ttm/ttm_lock.c:EXPORT_SYMBOL(ttm_vt_lock);
+#define ttm_vt_unlock amdttm_vt_unlock //ttm/ttm_lock.c:EXPORT_SYMBOL(ttm_vt_unlock);
+#define ttm_write_lock amdttm_write_lock //ttm/ttm_lock.c:EXPORT_SYMBOL(ttm_write_lock);
+#define ttm_write_unlock amdttm_write_unlock //ttm/ttm_lock.c:EXPORT_SYMBOL(ttm_write_unlock);
+#define ttm_get_kernel_zone_memory_size amdttm_get_kernel_zone_memory_size //ttm/ttm_memory.c:EXPORT_SYMBOL(ttm_get_kernel_zone_memory_size);
+#define ttm_mem_global_alloc amdttm_mem_global_alloc //ttm/ttm_memory.c:EXPORT_SYMBOL(ttm_mem_global_alloc);
+#define ttm_mem_global_free amdttm_mem_global_free //ttm/ttm_memory.c:EXPORT_SYMBOL(ttm_mem_global_free);
+#define ttm_mem_global_init amdttm_mem_global_init //ttm/ttm_memory.c:EXPORT_SYMBOL(ttm_mem_global_init);
+#define ttm_mem_global_release amdttm_mem_global_release //ttm/ttm_memory.c:EXPORT_SYMBOL(ttm_mem_global_release);
+#define ttm_round_pot amdttm_round_pot //ttm/ttm_memory.c:EXPORT_SYMBOL(ttm_round_pot);
+#define ttm_base_object_init amdttm_base_object_init //ttm/ttm_object.c:EXPORT_SYMBOL(ttm_base_object_init);
+#define ttm_base_object_lookup amdttm_base_object_lookup //ttm/ttm_object.c:EXPORT_SYMBOL(ttm_base_object_lookup);
+#define ttm_base_object_lookup_for_ref amdttm_base_object_lookup_for_ref //ttm/ttm_object.c:EXPORT_SYMBOL(ttm_base_object_lookup_for_ref);
+#define ttm_base_object_unref amdttm_base_object_unref //ttm/ttm_object.c:EXPORT_SYMBOL(ttm_base_object_unref);
+#define ttm_object_device_init amdttm_object_device_init //ttm/ttm_object.c:EXPORT_SYMBOL(ttm_object_device_init);
+#define ttm_object_device_release amdttm_object_device_release //ttm/ttm_object.c:EXPORT_SYMBOL(ttm_object_device_release);
+#define ttm_object_file_init amdttm_object_file_init //ttm/ttm_object.c:EXPORT_SYMBOL(ttm_object_file_init);
+#define ttm_object_file_release amdttm_object_file_release //ttm/ttm_object.c:EXPORT_SYMBOL(ttm_object_file_release);
+#define ttm_prime_object_init amdttm_prime_object_init //ttm/ttm_object.c:EXPORT_SYMBOL(ttm_prime_object_init);
+#define ttm_ref_object_add amdttm_ref_object_add //ttm/ttm_object.c:EXPORT_SYMBOL(ttm_ref_object_add);
+#define ttm_ref_object_base_unref amdttm_ref_object_base_unref //ttm/ttm_object.c:EXPORT_SYMBOL(ttm_ref_object_base_unref);
+#define ttm_ref_object_exists amdttm_ref_object_exists //ttm/ttm_object.c:EXPORT_SYMBOL(ttm_ref_object_exists);
+#define ttm_prime_fd_to_handle amdttm_prime_fd_to_handle //ttm/ttm_object.c:EXPORT_SYMBOL_GPL(ttm_prime_fd_to_handle);
+#define ttm_prime_handle_to_fd amdttm_prime_handle_to_fd //ttm/ttm_object.c:EXPORT_SYMBOL_GPL(ttm_prime_handle_to_fd);
+#define ttm_page_alloc_debugfs amdttm_page_alloc_debugfs //ttm/ttm_page_alloc.c:EXPORT_SYMBOL(ttm_page_alloc_debugfs);
+#define ttm_pool_populate amdttm_pool_populate //ttm/ttm_page_alloc.c:EXPORT_SYMBOL(ttm_pool_populate);
+#define ttm_pool_unpopulate amdttm_pool_unpopulate //ttm/ttm_page_alloc.c:EXPORT_SYMBOL(ttm_pool_unpopulate);
+#define ttm_populate_and_map_pages amdttm_populate_and_map_pages //ttm/ttm_page_alloc.c:EXPORT_SYMBOL(ttm_populate_and_map_pages);
+#define ttm_unmap_and_unpopulate_pages amdttm_unmap_and_unpopulate_pages //ttm/ttm_page_alloc.c:EXPORT_SYMBOL(ttm_unmap_and_unpopulate_pages);
+#define ttm_dma_page_alloc_debugfs amdttm_dma_page_alloc_debugfs //ttm/ttm_page_alloc_dma.c:EXPORT_SYMBOL_GPL(ttm_dma_page_alloc_debugfs);
+#define ttm_dma_populate amdttm_dma_populate //ttm/ttm_page_alloc_dma.c:EXPORT_SYMBOL_GPL(ttm_dma_populate);
+#define ttm_dma_unpopulate amdttm_dma_unpopulate //ttm/ttm_page_alloc_dma.c:EXPORT_SYMBOL_GPL(ttm_dma_unpopulate);
+#define ttm_dma_tt_fini amdttm_dma_tt_fini //ttm/ttm_tt.c:EXPORT_SYMBOL(ttm_dma_tt_fini);
+#define ttm_dma_tt_init amdttm_dma_tt_init //ttm/ttm_tt.c:EXPORT_SYMBOL(ttm_dma_tt_init);
+#define ttm_tt_bind amdttm_tt_bind //ttm/ttm_tt.c:EXPORT_SYMBOL(ttm_tt_bind);
+#define ttm_tt_fini amdttm_tt_fini //ttm/ttm_tt.c:EXPORT_SYMBOL(ttm_tt_fini);
+#define ttm_tt_init amdttm_tt_init //ttm/ttm_tt.c:EXPORT_SYMBOL(ttm_tt_init);
+#define ttm_tt_set_placement_caching amdttm_tt_set_placement_caching //ttm/ttm_tt.c:EXPORT_SYMBOL(ttm_tt_set_placement_caching);
+#define ttm_agp_tt_create amdttm_agp_tt_create //ttm/ttm_agp_backend.c:EXPORT_SYMBOL(ttm_agp_tt_create);
+#define ttm_agp_tt_populate amdttm_agp_tt_populate //ttm/ttm_agp_backend.c:EXPORT_SYMBOL(ttm_agp_tt_populate);
+#define ttm_agp_tt_unpopulate amdttm_agp_tt_unpopulate //ttm/ttm_agp_backend.c:EXPORT_SYMBOL(ttm_agp_tt_unpopulate);
+#define ttm_bo_acc_size amdttm_bo_acc_size //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_acc_size);
+#define ttm_bo_add_to_lru amdttm_bo_add_to_lru //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_add_to_lru);
+#define ttm_bo_clean_mm amdttm_bo_clean_mm //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_clean_mm);
+#define ttm_bo_create amdttm_bo_create //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_create);
+#define ttm_bo_del_sub_from_lru amdttm_bo_del_sub_from_lru //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_del_sub_from_lru);
+#define ttm_bo_device_init amdttm_bo_device_init //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_device_init);
+#define ttm_bo_device_release amdttm_bo_device_release //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_device_release);
+#define ttm_bo_dma_acc_size amdttm_bo_dma_acc_size //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_dma_acc_size);
+#define ttm_bo_evict_mm amdttm_bo_evict_mm //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_evict_mm);
+#define ttm_bo_eviction_valuable amdttm_bo_eviction_valuable //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_eviction_valuable);
+#define ttm_bo_global_init amdttm_bo_global_init //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_global_init);
+#define ttm_bo_global_release amdttm_bo_global_release //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_global_release);
+#define ttm_bo_init amdttm_bo_init //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_init);
+#define ttm_bo_init_mm amdttm_bo_init_mm //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_init_mm);
+#define ttm_bo_init_reserved amdttm_bo_init_reserved //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_init_reserved);
+#define ttm_bo_lock_delayed_workqueue amdttm_bo_lock_delayed_workqueue //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_lock_delayed_workqueue);
+#define ttm_bo_mem_compat amdttm_bo_mem_compat //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_mem_compat);
+#define ttm_bo_mem_put amdttm_bo_mem_put //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_mem_put);
+#define ttm_bo_mem_space amdttm_bo_mem_space //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_mem_space);
+#define ttm_bo_move_to_lru_tail amdttm_bo_move_to_lru_tail //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_move_to_lru_tail);
+#define ttm_bo_swapout_all amdttm_bo_swapout_all //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_swapout_all);
+#define ttm_bo_synccpu_write_grab amdttm_bo_synccpu_write_grab //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_synccpu_write_grab);
+#define ttm_bo_synccpu_write_release amdttm_bo_synccpu_write_release //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_synccpu_write_release);
+#define ttm_bo_unlock_delayed_workqueue amdttm_bo_unlock_delayed_workqueue //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_unlock_delayed_workqueue);
+#define ttm_bo_unmap_virtual amdttm_bo_unmap_virtual //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_unmap_virtual);
+#define ttm_bo_unref amdttm_bo_unref //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_unref);
+#define ttm_bo_validate amdttm_bo_validate //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_validate);
+#define ttm_bo_wait amdttm_bo_wait //ttm/ttm_bo.c:EXPORT_SYMBOL(ttm_bo_wait);
+#define ttm_bo_manager_func amdttm_bo_manager_func //ttm/ttm_bo_manager.c:EXPORT_SYMBOL(ttm_bo_manager_func);
+#define ttm_bo_kmap amdttm_bo_kmap //ttm/ttm_bo_util.c:EXPORT_SYMBOL(ttm_bo_kmap);
+#define ttm_bo_kunmap amdttm_bo_kunmap //ttm/ttm_bo_util.c:EXPORT_SYMBOL(ttm_bo_kunmap);
+#define ttm_bo_move_accel_cleanup amdttm_bo_move_accel_cleanup //ttm/ttm_bo_util.c:EXPORT_SYMBOL(ttm_bo_move_accel_cleanup);
+#define ttm_bo_move_memcpy amdttm_bo_move_memcpy //ttm/ttm_bo_util.c:EXPORT_SYMBOL(ttm_bo_move_memcpy);
+#define ttm_bo_move_ttm amdttm_bo_move_ttm //ttm/ttm_bo_util.c:EXPORT_SYMBOL(ttm_bo_move_ttm);
+#define ttm_bo_pipeline_move amdttm_bo_pipeline_move //ttm/ttm_bo_util.c:EXPORT_SYMBOL(ttm_bo_pipeline_move);
+#define ttm_io_prot amdttm_io_prot //ttm/ttm_bo_util.c:EXPORT_SYMBOL(ttm_io_prot);
+#define ttm_mem_io_free amdttm_mem_io_free //ttm/ttm_bo_util.c:EXPORT_SYMBOL(ttm_mem_io_free);
+#define ttm_mem_io_lock amdttm_mem_io_lock //ttm/ttm_bo_util.c:EXPORT_SYMBOL(ttm_mem_io_lock);
+#define ttm_mem_io_reserve amdttm_mem_io_reserve //ttm/ttm_bo_util.c:EXPORT_SYMBOL(ttm_mem_io_reserve);
+#define ttm_mem_io_unlock amdttm_mem_io_unlock //ttm/ttm_bo_util.c:EXPORT_SYMBOL(ttm_mem_io_unlock);
+#define ttm_bo_default_io_mem_pfn amdttm_bo_default_io_mem_pfn //ttm/ttm_bo_vm.c:EXPORT_SYMBOL(ttm_bo_default_io_mem_pfn);
+#define ttm_bo_mmap amdttm_bo_mmap //ttm/ttm_bo_vm.c:EXPORT_SYMBOL(ttm_bo_mmap);
+#define ttm_fbdev_mmap amdttm_fbdev_mmap //ttm/ttm_bo_vm.c:EXPORT_SYMBOL(ttm_fbdev_mmap);
+#define ttm_trace_dma_map amdttm_trace_dma_map //ttm/ttm_debug.c:EXPORT_SYMBOL(ttm_trace_dma_map);
+#define ttm_trace_dma_unmap amdttm_trace_dma_unmap //ttm/ttm_debug.c:EXPORT_SYMBOL(ttm_trace_dma_unmap);
+#define ttm_eu_backoff_reservation amdttm_eu_backoff_reservation //ttm/ttm_execbuf_util.c:EXPORT_SYMBOL(ttm_eu_backoff_reservation);
+#define ttm_eu_fence_buffer_objects amdttm_eu_fence_buffer_objects //ttm/ttm_execbuf_util.c:EXPORT_SYMBOL(ttm_eu_fence_buffer_objects);
+#define ttm_eu_reserve_buffers amdttm_eu_reserve_buffers //ttm/ttm_execbuf_util.c:EXPORT_SYMBOL(ttm_eu_reserve_buffers);
+#define ttm_lock_init amdttm_lock_init //ttm/ttm_lock.c:EXPORT_SYMBOL(ttm_lock_init);
+#define ttm_read_lock amdttm_read_lock //ttm/ttm_lock.c:EXPORT_SYMBOL(ttm_read_lock);
+#define ttm_read_unlock amdttm_read_unlock //ttm/ttm_lock.c:EXPORT_SYMBOL(ttm_read_unlock);
+#define ttm_suspend_lock amdttm_suspend_lock //ttm/ttm_lock.c:EXPORT_SYMBOL(ttm_suspend_lock);
+#define ttm_suspend_unlock amdttm_suspend_unlock //ttm/ttm_lock.c:EXPORT_SYMBOL(ttm_suspend_unlock);
+#define ttm_vt_lock amdttm_vt_lock //ttm/ttm_lock.c:EXPORT_SYMBOL(ttm_vt_lock);
+#define ttm_vt_unlock amdttm_vt_unlock //ttm/ttm_lock.c:EXPORT_SYMBOL(ttm_vt_unlock);
+#define ttm_write_lock amdttm_write_lock //ttm/ttm_lock.c:EXPORT_SYMBOL(ttm_write_lock);
+#define ttm_write_unlock amdttm_write_unlock //ttm/ttm_lock.c:EXPORT_SYMBOL(ttm_write_unlock);
+#define ttm_get_kernel_zone_memory_size amdttm_get_kernel_zone_memory_size //ttm/ttm_memory.c:EXPORT_SYMBOL(ttm_get_kernel_zone_memory_size);
+#define ttm_mem_global_alloc amdttm_mem_global_alloc //ttm/ttm_memory.c:EXPORT_SYMBOL(ttm_mem_global_alloc);
+#define ttm_mem_global_free amdttm_mem_global_free //ttm/ttm_memory.c:EXPORT_SYMBOL(ttm_mem_global_free);
+#define ttm_mem_global_init amdttm_mem_global_init //ttm/ttm_memory.c:EXPORT_SYMBOL(ttm_mem_global_init);
+#define ttm_mem_global_release amdttm_mem_global_release //ttm/ttm_memory.c:EXPORT_SYMBOL(ttm_mem_global_release);
+#define ttm_round_pot amdttm_round_pot //ttm/ttm_memory.c:EXPORT_SYMBOL(ttm_round_pot);
+#define ttm_base_object_init amdttm_base_object_init //ttm/ttm_object.c:EXPORT_SYMBOL(ttm_base_object_init);
+#define ttm_base_object_lookup amdttm_base_object_lookup //ttm/ttm_object.c:EXPORT_SYMBOL(ttm_base_object_lookup);
+#define ttm_base_object_lookup_for_ref amdttm_base_object_lookup_for_ref //ttm/ttm_object.c:EXPORT_SYMBOL(ttm_base_object_lookup_for_ref);
+#define ttm_base_object_unref amdttm_base_object_unref //ttm/ttm_object.c:EXPORT_SYMBOL(ttm_base_object_unref);
+#define ttm_object_device_init amdttm_object_device_init //ttm/ttm_object.c:EXPORT_SYMBOL(ttm_object_device_init);
+#define ttm_object_device_release amdttm_object_device_release //ttm/ttm_object.c:EXPORT_SYMBOL(ttm_object_device_release);
+#define ttm_object_file_init amdttm_object_file_init //ttm/ttm_object.c:EXPORT_SYMBOL(ttm_object_file_init);
+#define ttm_object_file_release amdttm_object_file_release //ttm/ttm_object.c:EXPORT_SYMBOL(ttm_object_file_release);
+#define ttm_prime_object_init amdttm_prime_object_init //ttm/ttm_object.c:EXPORT_SYMBOL(ttm_prime_object_init);
+#define ttm_ref_object_add amdttm_ref_object_add //ttm/ttm_object.c:EXPORT_SYMBOL(ttm_ref_object_add);
+#define ttm_ref_object_base_unref amdttm_ref_object_base_unref //ttm/ttm_object.c:EXPORT_SYMBOL(ttm_ref_object_base_unref);
+#define ttm_ref_object_exists amdttm_ref_object_exists //ttm/ttm_object.c:EXPORT_SYMBOL(ttm_ref_object_exists);
+#define ttm_prime_fd_to_handle amdttm_prime_fd_to_handle //ttm/ttm_object.c:EXPORT_SYMBOL_GPL(ttm_prime_fd_to_handle);
+#define ttm_prime_handle_to_fd amdttm_prime_handle_to_fd //ttm/ttm_object.c:EXPORT_SYMBOL_GPL(ttm_prime_handle_to_fd);
+#define ttm_page_alloc_debugfs amdttm_page_alloc_debugfs //ttm/ttm_page_alloc.c:EXPORT_SYMBOL(ttm_page_alloc_debugfs);
+#define ttm_pool_populate amdttm_pool_populate //ttm/ttm_page_alloc.c:EXPORT_SYMBOL(ttm_pool_populate);
+#define ttm_pool_unpopulate amdttm_pool_unpopulate //ttm/ttm_page_alloc.c:EXPORT_SYMBOL(ttm_pool_unpopulate);
+#define ttm_populate_and_map_pages amdttm_populate_and_map_pages //ttm/ttm_page_alloc.c:EXPORT_SYMBOL(ttm_populate_and_map_pages);
+#define ttm_unmap_and_unpopulate_pages amdttm_unmap_and_unpopulate_pages //ttm/ttm_page_alloc.c:EXPORT_SYMBOL(ttm_unmap_and_unpopulate_pages);
+#define ttm_dma_page_alloc_debugfs amdttm_dma_page_alloc_debugfs //ttm/ttm_page_alloc_dma.c:EXPORT_SYMBOL_GPL(ttm_dma_page_alloc_debugfs);
+#define ttm_dma_populate amdttm_dma_populate //ttm/ttm_page_alloc_dma.c:EXPORT_SYMBOL_GPL(ttm_dma_populate);
+#define ttm_dma_unpopulate amdttm_dma_unpopulate //ttm/ttm_page_alloc_dma.c:EXPORT_SYMBOL_GPL(ttm_dma_unpopulate);
+#define ttm_dma_tt_fini amdttm_dma_tt_fini //ttm/ttm_tt.c:EXPORT_SYMBOL(ttm_dma_tt_fini);
+#define ttm_dma_tt_init amdttm_dma_tt_init //ttm/ttm_tt.c:EXPORT_SYMBOL(ttm_dma_tt_init);
+#define ttm_tt_bind amdttm_tt_bind //ttm/ttm_tt.c:EXPORT_SYMBOL(ttm_tt_bind);
+#define ttm_tt_fini amdttm_tt_fini //ttm/ttm_tt.c:EXPORT_SYMBOL(ttm_tt_fini);
+#define ttm_tt_init amdttm_tt_init //ttm/ttm_tt.c:EXPORT_SYMBOL(ttm_tt_init);
+#define ttm_tt_set_placement_caching amdttm_tt_set_placement_caching //ttm/ttm_tt.c:EXPORT_SYMBOL(ttm_tt_set_placement_caching);
